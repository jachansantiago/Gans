{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select processing devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "from keras.layers import Activation, Reshape, UpSampling2D\n",
    "from keras.layers.convolutional import Conv2DTranspose\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_images = \"quickdraw_dataset/apple.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144722, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data = np.load(input_images)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144722, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data/255.\n",
    "data = np.reshape(data, (-1, 28, 28, 1))\n",
    "img_w, img_h = data.shape[1:3]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1089f46a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10133ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[4343, :,:, 0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create discrimanator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_builder(width=64, p=0.4):\n",
    "    \n",
    "    # define inputs\n",
    "    inputs = Input((img_w, img_h, 1))\n",
    "    \n",
    "    # Fisrt Convolutional layer:\n",
    "    conv1 = Conv2D(width*1, 5, strides=2, padding='same', activation='relu')(inputs)\n",
    "    conv1 = Dropout(p)(conv1)\n",
    "    \n",
    "    # Second Convolutional layer:\n",
    "    conv2 = Conv2D(width*2, 5, strides=2, padding='same', activation='relu')(conv1)\n",
    "    conv2 = Dropout(p)(conv2)\n",
    "    \n",
    "    # Third Convolutional layer:\n",
    "    conv3 = Conv2D(width*4, 5, strides=2, padding='same', activation='relu')(conv2)\n",
    "    conv3 = Dropout(p)(conv3)\n",
    "    \n",
    "    # Fourth Convolutional layer:\n",
    "    conv4 = Conv2D(width*8, 5, strides=1, padding='same', activation='relu')(conv3)\n",
    "    conv4 = Flatten()(Dropout(p)(conv4))\n",
    "    \n",
    "    # output layer\n",
    "    output = Dense(1, activation='sigmoid')(conv4)\n",
    "    \n",
    "    # model definition:\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=RMSprop(lr=0.0008, decay=6e-8, clipvalue=1),\n",
    "                      metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_builder(z_dim=100, width=64, p=0.4):\n",
    "    \n",
    "    # define inputs\n",
    "    inputs = Input((z_dim,))\n",
    "    \n",
    "    # First dense layer:\n",
    "    dense1 = Dense(7*7*width)(inputs)\n",
    "    dense1 = BatchNormalization(momentum=0.9)(dense1)\n",
    "    dense1 = Activation(activation='relu')(dense1)\n",
    "    dense1 = Reshape((7, 7, width))(dense1)\n",
    "    dense1 = Dropout(p)(dense1)\n",
    "    \n",
    "    # First Deconvolutional layer:\n",
    "    conv1 = UpSampling2D()(dense1)\n",
    "    conv1 = Conv2DTranspose(int(width/2), kernel_size=5, padding='same', activation=None)(conv1)\n",
    "    conv1 = BatchNormalization(momentum=0.9)(conv1)\n",
    "    conv1 = Activation(activation='relu')(conv1)\n",
    "    \n",
    "    # Second Deconvolutional layer:\n",
    "    conv2 = UpSampling2D()(conv1)\n",
    "    conv2 = Conv2DTranspose(int(width/4), kernel_size=5, padding='same', activation=None)(conv2)\n",
    "    conv2 = BatchNormalization(momentum=0.9)(conv2)\n",
    "    conv2 = Activation(activation='relu')(conv2)\n",
    "    \n",
    "    # Third Deconvolutional layer:\n",
    "    conv3 = Conv2DTranspose(int(width/8), kernel_size=5, padding='same', activation=None)(conv2)\n",
    "    conv3 = BatchNormalization(momentum=0.9)(conv3)\n",
    "    conv3 = Activation(activation='relu')(conv3)\n",
    "    \n",
    "    # output layer:\n",
    "    output = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(conv3)\n",
    "    \n",
    "    # model definition:\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 16)        12816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 8)         3208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         201       \n",
      "=================================================================\n",
      "Total params: 396,961\n",
      "Trainable params: 390,577\n",
      "Non-trainable params: 6,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = generator_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Adversarial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adversarial_builder(z_dim=100):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer= RMSprop(lr=0.0004, decay=3e-8, clipvalue=1.0),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_2 (Model)              (None, 28, 28, 1)         396961    \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 4311553   \n",
      "=================================================================\n",
      "Total params: 4,708,514\n",
      "Trainable params: 4,702,130\n",
      "Non-trainable params: 6,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adversarial_model = adversarial_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    \n",
    "    for l in net.layers:\n",
    "        l.trainable = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=2000, batch=128):\n",
    "    \n",
    "    d_metrics = []\n",
    "    a_metrics = []\n",
    "    \n",
    "    running_d_loss = 0\n",
    "    running_d_acc = 0\n",
    "    running_a_loss = 0\n",
    "    running_a_acc = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        real_imgs = np.reshape(data[np.random.choice(data.shape[0], batch, replace=False)], (batch, 28, 28, 1))\n",
    "        fake_imgs = generator.predict(np.random.uniform(-1.0, 1.0, size=[batch, 100]))\n",
    "\n",
    "        x = np.concatenate((real_imgs, fake_imgs))\n",
    "        \n",
    "        y = np.ones([2*batch, 1])\n",
    "        y[batch:, :] = 0\n",
    "        \n",
    "        make_trainable(discriminator, True)\n",
    "        \n",
    "        d_metrics.append(discriminator.train_on_batch(x, y))\n",
    "        running_d_loss += d_metrics[-1][0]\n",
    "        running_d_acc += d_metrics[-1][1]\n",
    "        \n",
    "        make_trainable(discriminator, False)\n",
    "        \n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch, 100])\n",
    "        y = np.ones([batch, 1])\n",
    "        \n",
    "        a_metrics.append(adversarial_model.train_on_batch(noise, y))\n",
    "        running_a_loss += a_metrics[-1][0]\n",
    "        running_a_acc += a_metrics[-1][1]\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print('Epoch #{}'.format(i+1))\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, running_d_loss/i, running_d_acc/i)\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, running_a_loss/i, running_a_acc/i)\n",
    "            print(log_mesg)\n",
    "\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            plt.figure(figsize=(5,5))\n",
    "\n",
    "            for k in range(gen_imgs.shape[0]):\n",
    "                plt.subplot(4, 4, k+1)\n",
    "                plt.imshow(gen_imgs[k, :, :, 0], cmap='gray')\n",
    "                plt.axis('off')\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return a_metrics, d_metrics\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KaiiDoZ/anaconda/envs/python3/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "a_metrics_complete, d_metrics_complete = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Generator': [metric[0] for metric in a_metrics_complete],\n",
    "        'Discriminator': [metric[0] for metric in d_metrics_complete],\n",
    "    }\n",
    ").plot(title='Training Loss', logy=True)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(\n",
    "    {\n",
    "        'Generator': [metric[1] for metric in a_metrics_complete],\n",
    "        'Discriminator': [metric[1] for metric in d_metrics_complete],\n",
    "    }\n",
    ").plot(title='Training Accuracy')\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
